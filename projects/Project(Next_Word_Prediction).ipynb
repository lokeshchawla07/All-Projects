{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMDnHawvHU8Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "9IAFYNZTHW_N",
    "outputId": "1bf0750e-f5ef-4b98-b8dc-2f9ebcf00952"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-b4b4ad50-64b6-4570-ae81-8d560dd70088\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-b4b4ad50-64b6-4570-ae81-8d560dd70088\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sherlock-holm.es_stories_plain-text_advs.txt to sherlock-holm.es_stories_plain-text_advs.txt\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "path_to_file = list(files.upload().keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IdbHyn8HajK"
   },
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open(path_to_file, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLZ-wi1sHyjB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSwoMJNIHq8_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9rGenWXHweK"
   },
   "outputs": [],
   "source": [
    "# Now letâ€™s tokenize the text to create a sequence of words:\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_7V4_AOH8nd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z27cjUCxIGPb"
   },
   "source": [
    "In the above code, the text is tokenized, which means it is divided into individual words or tokens. The â€˜Tokenizerâ€™ object is created, which will handle the tokenization process. The â€˜fit_on_textsâ€™ method of the tokenizer is called, passing the â€˜textâ€™ as input. This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index. The â€˜total_wordsâ€™ variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIijNRJMIG13"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAvy8IWjIKp_"
   },
   "source": [
    "Now letâ€™s create input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvdUwaOcILHQ"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzwG875cIVHB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMufmtDpIc6X"
   },
   "source": [
    "n the above code, the text data is split into lines using the â€˜\\nâ€™ character as a delimiter. For each line in the text, the â€˜texts_to_sequencesâ€™ method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary. The resulting token list is then iterated over using a for loop. For each iteration, a subsequence, or n-gram, of tokens is extracted, ranging from the beginning of the token list up to the current index â€˜iâ€™.\n",
    "\n",
    "This n-gram sequence represents the input context, with the last token being the target or predicted word. This n-gram sequence is then appended to the â€˜input_sequencesâ€™ list. This process is repeated for all lines in the text, generating multiple input-output sequences that will be used for training the next word prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6s5ZMoCIdiH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0hVU5RGIgjv"
   },
   "source": [
    "Now letâ€™s pad the input sequences to have equal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZArEjzR9IhAR"
   },
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqRQcZfzIzFr"
   },
   "source": [
    "In the above code, the input sequences are padded to ensure all sequences have the same length. The variable â€˜max_sequence_lenâ€™ is assigned the maximum length among all the input sequences. The â€˜pad_sequencesâ€™ function is used to pad or truncate the input sequences to match this maximum length.\n",
    "\n",
    "\n",
    "\n",
    "The â€˜pad_sequencesâ€™ function takes the input_sequences list, sets the maximum length to â€˜max_sequence_lenâ€™, and specifies that the padding should be added at the beginning of each sequence using the â€˜padding=preâ€™ argument. Finally, the input sequences are converted into a numpy array to facilitate further processing.\n",
    "\n",
    "\n",
    "\n",
    "Now letâ€™s split the sequences into input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0trQJvuIvbR"
   },
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfAAC5xoI5CO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbLxbPoTI9G2"
   },
   "source": [
    "In the above code, the input sequences are split into two arrays, â€˜Xâ€™ and â€˜yâ€™, to create the input and output for training the next word prediction model. The â€˜Xâ€™ array is assigned the values of all rows in the â€˜input_sequencesâ€™ array except for the last column. It means that â€˜Xâ€™ contains all the tokens in each sequence except for the last one, representing the input context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_ERB72NI99q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbiWxp4tJCJQ"
   },
   "source": [
    "On the other hand, the â€˜yâ€™ array is assigned the values of the last column in the â€˜input_sequencesâ€™ array, which represents the target or predicted word.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now letâ€™s convert the output to one-hot encode vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bX0GaQiLJDGd"
   },
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "672Ez6i_JF3d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ35Aoj2JMrJ"
   },
   "source": [
    "In the above code, we are converting the output array into a suitable format for training a model, where each target word is represented as a binary vector.\n",
    "\n",
    "\n",
    "\n",
    "Now letâ€™s build a neural network architecture to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ypga52S1JNqS",
    "outputId": "090e80b4-3ab9-4cad-c2d9-3d7d4d908f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8200)              1238200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,208,800\n",
      "Trainable params: 2,208,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yz2yCpb3JSrq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymZF8Cf7JhJP"
   },
   "source": [
    "The code above defines the model architecture for the next word prediction model. The â€˜Sequentialâ€™ model is created, which represents a linear stack of layers. The first layer added to the model is the â€˜Embeddingâ€™ layer, which is responsible for converting the input sequences into dense vectors of fixed size. It takes three arguments:\n",
    "\n",
    "(1). â€˜total_wordsâ€™, which represents the total number of distinct words in the vocabulary;\n",
    "\n",
    "(2). â€˜100â€™, which denotes the dimensionality of the word embeddings;\n",
    "\n",
    "\n",
    "(3). and â€˜input_lengthâ€™, which specifies the length of the input sequences.\n",
    "\n",
    "\n",
    "\n",
    "The next layer added is the â€˜LSTMâ€™ layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data. It has 150 units, which means it will learn 150 internal representations or memory cells.\n",
    "\n",
    "\n",
    "\n",
    "Finally, the â€˜Denseâ€™ layer is added, which is a fully connected layer that produces the output predictions. It has â€˜total_wordsâ€™ units and uses the â€˜softmaxâ€™ activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence.\n",
    "\n",
    "\n",
    "\n",
    "Now letâ€™s compile and train the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iriPh7zqJ_OK",
    "outputId": "2e1394cb-edfb-4f16-d5f8-cb27b01a04cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3010/3010 [==============================] - 122s 40ms/step - loss: 6.2516 - accuracy: 0.0744\n",
      "Epoch 2/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 5.5331 - accuracy: 0.1211\n",
      "Epoch 3/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 5.1481 - accuracy: 0.1449\n",
      "Epoch 4/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 4.8183 - accuracy: 0.1637\n",
      "Epoch 5/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 4.5071 - accuracy: 0.1812\n",
      "Epoch 6/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 4.2139 - accuracy: 0.2017\n",
      "Epoch 7/100\n",
      "3010/3010 [==============================] - 119s 40ms/step - loss: 3.9376 - accuracy: 0.2267\n",
      "Epoch 8/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 3.6748 - accuracy: 0.2572\n",
      "Epoch 9/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 3.4290 - accuracy: 0.2919\n",
      "Epoch 10/100\n",
      "3010/3010 [==============================] - 119s 39ms/step - loss: 3.1992 - accuracy: 0.3265\n",
      "Epoch 11/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 2.9870 - accuracy: 0.3616\n",
      "Epoch 12/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 2.7929 - accuracy: 0.3967\n",
      "Epoch 13/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 2.6109 - accuracy: 0.4299\n",
      "Epoch 14/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 2.4465 - accuracy: 0.4615\n",
      "Epoch 15/100\n",
      "3010/3010 [==============================] - 119s 39ms/step - loss: 2.2930 - accuracy: 0.4931\n",
      "Epoch 16/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 2.1554 - accuracy: 0.5202\n",
      "Epoch 17/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 2.0268 - accuracy: 0.5467\n",
      "Epoch 18/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 1.9096 - accuracy: 0.5718\n",
      "Epoch 19/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 1.8027 - accuracy: 0.5941\n",
      "Epoch 20/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 1.7053 - accuracy: 0.6154\n",
      "Epoch 21/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 1.6159 - accuracy: 0.6361\n",
      "Epoch 22/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 1.5322 - accuracy: 0.6538\n",
      "Epoch 23/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 1.4567 - accuracy: 0.6695\n",
      "Epoch 24/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 1.3885 - accuracy: 0.6849\n",
      "Epoch 25/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 1.3242 - accuracy: 0.6983\n",
      "Epoch 26/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 1.2667 - accuracy: 0.7099\n",
      "Epoch 27/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 1.2115 - accuracy: 0.7234\n",
      "Epoch 28/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 1.1602 - accuracy: 0.7348\n",
      "Epoch 29/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 1.1168 - accuracy: 0.7437\n",
      "Epoch 30/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 1.0744 - accuracy: 0.7534\n",
      "Epoch 31/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 1.0358 - accuracy: 0.7612\n",
      "Epoch 32/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.9986 - accuracy: 0.7691\n",
      "Epoch 33/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 0.9649 - accuracy: 0.7782\n",
      "Epoch 34/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.9370 - accuracy: 0.7829\n",
      "Epoch 35/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 0.9048 - accuracy: 0.7907\n",
      "Epoch 36/100\n",
      "3010/3010 [==============================] - 119s 40ms/step - loss: 0.8792 - accuracy: 0.7958\n",
      "Epoch 37/100\n",
      "3010/3010 [==============================] - 119s 39ms/step - loss: 0.8557 - accuracy: 0.8007\n",
      "Epoch 38/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.8370 - accuracy: 0.8046\n",
      "Epoch 39/100\n",
      "3010/3010 [==============================] - 116s 39ms/step - loss: 0.8136 - accuracy: 0.8108\n",
      "Epoch 40/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.7954 - accuracy: 0.8132\n",
      "Epoch 41/100\n",
      "3010/3010 [==============================] - 113s 38ms/step - loss: 0.7770 - accuracy: 0.8184\n",
      "Epoch 42/100\n",
      "3010/3010 [==============================] - 116s 39ms/step - loss: 0.7637 - accuracy: 0.8197\n",
      "Epoch 43/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.7459 - accuracy: 0.8235\n",
      "Epoch 44/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.7316 - accuracy: 0.8274\n",
      "Epoch 45/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 0.7226 - accuracy: 0.8277\n",
      "Epoch 46/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.7072 - accuracy: 0.8322\n",
      "Epoch 47/100\n",
      "3010/3010 [==============================] - 116s 39ms/step - loss: 0.6964 - accuracy: 0.8335\n",
      "Epoch 48/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.6835 - accuracy: 0.8358\n",
      "Epoch 49/100\n",
      "3010/3010 [==============================] - 116s 39ms/step - loss: 0.6772 - accuracy: 0.8367\n",
      "Epoch 50/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.6664 - accuracy: 0.8400\n",
      "Epoch 51/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.6604 - accuracy: 0.8395\n",
      "Epoch 52/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.6500 - accuracy: 0.8424\n",
      "Epoch 53/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.6445 - accuracy: 0.8438\n",
      "Epoch 54/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.6349 - accuracy: 0.8450\n",
      "Epoch 55/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.6279 - accuracy: 0.8469\n",
      "Epoch 56/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 0.6260 - accuracy: 0.8460\n",
      "Epoch 57/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.6190 - accuracy: 0.8478\n",
      "Epoch 58/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.6114 - accuracy: 0.8502\n",
      "Epoch 59/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.6050 - accuracy: 0.8512\n",
      "Epoch 60/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5985 - accuracy: 0.8532\n",
      "Epoch 61/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.5986 - accuracy: 0.8527\n",
      "Epoch 62/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5975 - accuracy: 0.8509\n",
      "Epoch 63/100\n",
      "3010/3010 [==============================] - 116s 38ms/step - loss: 0.5871 - accuracy: 0.8542\n",
      "Epoch 64/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5877 - accuracy: 0.8525\n",
      "Epoch 65/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 0.5802 - accuracy: 0.8548\n",
      "Epoch 66/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5747 - accuracy: 0.8576\n",
      "Epoch 67/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 0.5729 - accuracy: 0.8570\n",
      "Epoch 68/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5783 - accuracy: 0.8544\n",
      "Epoch 69/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5697 - accuracy: 0.8561\n",
      "Epoch 70/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5661 - accuracy: 0.8572\n",
      "Epoch 71/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5690 - accuracy: 0.8561\n",
      "Epoch 72/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5631 - accuracy: 0.8573\n",
      "Epoch 73/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5590 - accuracy: 0.8586\n",
      "Epoch 74/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5593 - accuracy: 0.8574\n",
      "Epoch 75/100\n",
      "3010/3010 [==============================] - 116s 39ms/step - loss: 0.5585 - accuracy: 0.8565\n",
      "Epoch 76/100\n",
      "3010/3010 [==============================] - 119s 39ms/step - loss: 0.5543 - accuracy: 0.8583\n",
      "Epoch 77/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5505 - accuracy: 0.8595\n",
      "Epoch 78/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5546 - accuracy: 0.8576\n",
      "Epoch 79/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5545 - accuracy: 0.8572\n",
      "Epoch 80/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5456 - accuracy: 0.8601\n",
      "Epoch 81/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5473 - accuracy: 0.8593\n",
      "Epoch 82/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5516 - accuracy: 0.8576\n",
      "Epoch 83/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5469 - accuracy: 0.8581\n",
      "Epoch 84/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5422 - accuracy: 0.8596\n",
      "Epoch 85/100\n",
      "3010/3010 [==============================] - 116s 39ms/step - loss: 0.5400 - accuracy: 0.8602\n",
      "Epoch 86/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5397 - accuracy: 0.8601\n",
      "Epoch 87/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5368 - accuracy: 0.8594\n",
      "Epoch 88/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5435 - accuracy: 0.8579\n",
      "Epoch 89/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 0.5266 - accuracy: 0.8644\n",
      "Epoch 90/100\n",
      "3010/3010 [==============================] - 119s 40ms/step - loss: 0.5394 - accuracy: 0.8591\n",
      "Epoch 91/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5353 - accuracy: 0.8604\n",
      "Epoch 92/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 0.5301 - accuracy: 0.8612\n",
      "Epoch 93/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 0.5328 - accuracy: 0.8599\n",
      "Epoch 94/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 0.5362 - accuracy: 0.8595\n",
      "Epoch 95/100\n",
      "3010/3010 [==============================] - 121s 40ms/step - loss: 0.5338 - accuracy: 0.8607\n",
      "Epoch 96/100\n",
      "3010/3010 [==============================] - 126s 42ms/step - loss: 0.5354 - accuracy: 0.8593\n",
      "Epoch 97/100\n",
      "3010/3010 [==============================] - 122s 40ms/step - loss: 0.5254 - accuracy: 0.8617\n",
      "Epoch 98/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 0.5344 - accuracy: 0.8586\n",
      "Epoch 99/100\n",
      "3010/3010 [==============================] - 124s 41ms/step - loss: 0.5276 - accuracy: 0.8607\n",
      "Epoch 100/100\n",
      "3010/3010 [==============================] - 122s 40ms/step - loss: 0.5313 - accuracy: 0.8601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x792530a71f60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBo7NtQoKEq6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMpZsn1mKYzI"
   },
   "source": [
    "In the above code, the model is being compiled and trained. The â€˜compileâ€™ method configures the model for training. The â€˜lossâ€™ parameter is set to â€˜categorical_crossentropyâ€™, a commonly used loss function for multi-class classification problems. The â€˜optimizerâ€™ parameter is set to â€˜adamâ€™, an optimization algorithm that adapts the learning rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agWTKcEvKXCD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ4jN3LdKhSC"
   },
   "source": [
    "The â€˜metricsâ€™ parameter is set to â€˜accuracyâ€™ to monitor the accuracy during training. After compiling the model, the â€˜fitâ€™ method is called to train the model on the input sequences â€˜Xâ€™ and the corresponding output â€˜yâ€™. The â€˜epochsâ€™ parameter specifies the number of times the training process will iterate over the entire dataset. The â€˜verboseâ€™ parameter is set to â€˜1â€™ to display the training process.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The above code will take more than an hour to execute. Once the code is executed, hereâ€™s how we can generate the next word predictions using our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYTkpAAeKXEe",
    "outputId": "354855bd-a5db-43df-a602-cf4061297c12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Are you indeed now cried\n"
     ]
    }
   ],
   "source": [
    "# seed_text = \"I will leave if they\"\n",
    "seed_text = \"Are you\"\n",
    "next_words = 3\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xdwy4-haKXG_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCmG9MT9LK5j"
   },
   "source": [
    "The above code generates the next word predictions based on a given seed text. The â€˜seed_textâ€™ variable holds the initial text. The â€˜next_wordsâ€™ variable determines the number of predictions to be generated. Inside the for loop, the â€˜seed_textâ€™ is converted into a sequence of tokens using the tokenizer. The token sequence is padded to match the maximum sequence length.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The model predicts the next word by calling the â€˜predictâ€™ method on the model with the padded token sequence. The predicted word is obtained by finding the word with the highest probability score using â€˜np.argmaxâ€™. Then, the predicted word is appended to the â€˜seed_textâ€™, and the process is repeated for the desired number of â€˜next_wordsâ€™. Finally, the â€˜seed_textâ€™ is printed, which contains the initial text followed by the generated predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bd4HsJmAKXJZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lH1pL-yUKXLw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tl7Yra1WKXOS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NBOXUx7KXQt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkQj10s0KXTD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKPhj7RrKXVy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1iM-svmKXYg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6ruJdRvKXba"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCc2b0oDKXe8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
